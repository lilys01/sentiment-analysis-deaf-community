{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- James Larsen\n",
    "- Alejandro Servin\n",
    "- Lily Steiner\n",
    "- Mayra Trejo\n",
    "- Lucy Lennemann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='research_question'></a>\n",
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How has the sentiment of the language surrounding Deafness used by popular online news sources (ABC, New York Times, USA Today, The Guardian, Associated Press) changed since the 80s?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got our datasets by scraping public APIs for news sources. We queried the APIs for articles related to deafness. The APIs would return a list of URLs for articles related to our search. We then would scrape the article text and other relevant information from the URL's website.\n",
    "\n",
    "__AP News Articles, ABC News Articles, USA Today Articles__\n",
    "- datasets/ap_data.json and datasets/abc_data.json, datasets/usa_data.json\n",
    "- These datasets were made by getting article URLs from the Google Custom Search API and then scraping the articles from their news sites\n",
    "    - https://developers.google.com/custom-search/v1/introduction\n",
    "- 380 AP articles, 160 ABC articles, 1300 USA Today articles\n",
    "\n",
    "__New York Times Articles__\n",
    "- datasets/nyt_data.json\n",
    "- This dataset was made by getting article URLs from the NYT API and then scraping the articles from the NYT site\n",
    "    - https://developer.nytimes.com/\n",
    "- 750 articles\n",
    "\n",
    "__The Guardian Articles__\n",
    "- datasets/guard_data.json\n",
    "- This dataset was made by getting article URLs from The Guardian API and then scraping the articles from The Guardian website\n",
    "    - https://open-platform.theguardian.com/\n",
    "- 7000 articles\n",
    "\n",
    "These datasets should be easy to combine due to us collecting the same information for each article. The information we collected was the news source, the URL, the headline, the publishing date, and the article text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages, some will be used during analysis\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import unicodedata\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datasets\n",
    "# Import ABC Dataset\n",
    "with open('dataset/abc_data.json') as abc_ds:\n",
    "    abc_data=json.load(abc_ds)\n",
    "    \n",
    "# Import Alternative Press Dataset\n",
    "with open('dataset/ap_data.json') as ap_ds:\n",
    "    ap_data=json.load(ap_ds)\n",
    "\n",
    "# Import The Guardian Dataset\n",
    "with open('dataset/guard_data.json') as guard_ds:\n",
    "    guard_data=json.load(guard_ds)\n",
    "    \n",
    "# Import New York Times Dataset\n",
    "with open('dataset/nyt_data.json') as nyt_ds:\n",
    "    nyt_data=json.load(nyt_ds)\n",
    "\n",
    "# Import USA Today Dataset\n",
    "with open('dataset/usa_data.json') as usa_ds:\n",
    "    usa_data=json.load(usa_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datasets to dataforms\n",
    "abc_df = pd.read_json('dataset/abc_data.json')\n",
    "ap_df = pd.read_json('dataset/ap_data.json') \n",
    "guard_df = pd.read_json('dataset/guard_data.json')\n",
    "nyt_df = pd.read_json('dataset/nyt_data.json') \n",
    "usa_df = pd.read_json('dataset/usa_data.json') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set row and column display\n",
    "pd.options.display.max_rows=6\n",
    "pd.options.display.max_columns=5\n",
    "\n",
    "#Used to look for text errors reverted for cleaning\n",
    "#pd.options.display.max_colwidth=None \n",
    "\n",
    "pd.options.display.max_colwidth=40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We are reordering the columns of all the dataframes so that they match.\n",
    "2. We are converting the date strings into pd.datetime format\n",
    "3. We are removing all articles before 1980-01-01\n",
    "4. We are removing unicode artifacts from the text using unicodedata.normalize\n",
    "5. We are removing any extraneous articles\n",
    "6. We are removing any extraneous pieces of article text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize dataframe\n",
    "abc_df                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize columns\n",
    "abc_df = abc_df[['headline','date','source','url','text']]\n",
    "\n",
    "# Convert 'date' to datetime format and only visualize date\n",
    "abc_df['date'] = pd.to_datetime(abc_df['date'], errors='coerce')\n",
    "\n",
    "# Remove articles before 1980-01-01\n",
    "abc_df = abc_df[~(abc_df['date']<='1980-01-01')]\n",
    "\n",
    "# Drop 'source' column for easier visualization\n",
    "abc_df.drop(columns=['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for null values\n",
    "abc_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb for unique values in the 'headline' column\n",
    "abc_df['headline'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb text for unique values in the 'text' column\n",
    "abc_df['text'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean text\n",
    "abc_df['text'] = abc_df['text'].apply(lambda t: unicodedata.normalize('NFKD', t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Press Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize dataframe\n",
    "ap_df                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize columns\n",
    "ap_df = ap_df[['headline','date','source','url','text']]\n",
    "\n",
    "# Convert 'date' to datetime format and only visualize date\n",
    "ap_df['date'] = pd.to_datetime(ap_df['date'])\n",
    "\n",
    "#Remove articles before 1980-01-01\n",
    "ap_df = ap_df[~(ap_df['date']<='1980-01-01')]\n",
    "\n",
    "# Drop 'source' column for easier visualization\n",
    "ap_df.drop(columns=['source'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for null values\n",
    "ap_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb for unique values in the 'headline' column\n",
    "ap_df['headline'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb for unique values in the 'text' column\n",
    "ap_df['text'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove articles that report sports scores\n",
    "ap_df = ap_df[ap_df['headline'].str.contains(\"Monday's Scores|Tuesday's Scores|Wednesday's Scores|Thursday's Scores|Friday's Scores|Saturday's Scores|Sunday's Scores\")==False]\n",
    "\n",
    "#Clean text\n",
    "ap_df['text'] = ap_df['text'].apply(lambda t: unicodedata.normalize('NFKD', t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Guardian Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize dataframe\n",
    "guard_df                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize columns\n",
    "guard_df = guard_df[['headline','date','source','url','text']]\n",
    "\n",
    "# Convert 'date' to datetime format and only visualize date\n",
    "guard_df['date'] = pd.to_datetime(guard_df['date'])\n",
    "\n",
    "#Remove articles before 1980-01-01\n",
    "guard_df = guard_df[~(guard_df['date']<='1980-01-01')]\n",
    "\n",
    "# Drop 'source' column for easier visualization\n",
    "guard_df.drop(columns=['source'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for null values\n",
    "guard_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb for unique values in the 'headline' column\n",
    "guard_df['headline'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb for unique values in the 'text' column\n",
    "guard_df['text'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean text\n",
    "guard_df['text'] = guard_df['text'].apply(lambda t: unicodedata.normalize('NFKD', t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New York Times Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize dataframe\n",
    "nyt_df                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize columns\n",
    "nyt_df = nyt_df[['headline','date','source','url','text']]\n",
    "\n",
    "# Convert 'date' to datetime format and only visualize date\n",
    "nyt_df['date'] = pd.to_datetime(nyt_df['date'])\n",
    "\n",
    "#Remove articles before 1980-01-01\n",
    "nyt_df = nyt_df[~(nyt_df['date']<='1980-01-01')]\n",
    "\n",
    "# Drop 'source' column for easier visualization\n",
    "nyt_df.drop(columns=['source'])\n",
    "\n",
    "# Visualize 'text' to search for errors\n",
    "#nyt_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look for null values\n",
    "nyt_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb for unique values in the 'headline' column\n",
    "nyt_df['headline'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb for unique values in teh 'text' column\n",
    "nyt_df['text'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the archive heading from the article text\n",
    "nyt_df['text'] = nyt_df['text'].str.replace('Credit...The New York Times Archives.*improve these archived versions.', '', regex=True)\n",
    "\n",
    "#Removing weird formatting stuff\n",
    "nyt_df['text'] = nyt_df['text'].str.replace('{.*?}', '', regex=True)\n",
    "\n",
    "#Clean text\n",
    "nyt_df['text'] = nyt_df['text'].apply(lambda t: unicodedata.normalize('NFKD', t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USA Today Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe using dataset\n",
    "\n",
    "#visualize dataframe\n",
    "usa_df                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize columns\n",
    "usa_df = usa_df[['headline','date','source','url','text']]\n",
    "\n",
    "# Convert 'date' to datetime format and only visualize date\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "usa_df['date'] = usa_df['date'].str.extract(r'Published:? (.*?)(?:Updated:?.*)?$')\n",
    "usa_df['date'] = usa_df['date'].str.replace('ET', '')\n",
    "usa_df['date'] = pd.to_datetime(usa_df['date'])\n",
    "\n",
    "pd.options.mode.chained_assignment = 'warn'\n",
    "\n",
    "# Remove articles before 1980-01-01\n",
    "usa_df = usa_df[~(usa_df['date']<='1980-01-01')]\n",
    "\n",
    "# Drop 'source' column for easier visualization\n",
    "usa_df.drop(columns=['source'])\n",
    "\n",
    "#Find data types\n",
    "usa_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for null values\n",
    "print(usa_df.isnull().sum())\n",
    "usa_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb for unique values in the 'headline' column\n",
    "usa_df['headline'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb for unique values in the 'text' column\n",
    "usa_df['text'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Text\n",
    "usa_df['text'] = usa_df['text'].apply(lambda t: unicodedata.normalize('NFKD', t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes for function iteration\n",
    "df_list = [abc_df, ap_df, guard_df, nyt_df, usa_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat(df_list)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
