{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - EDA Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- James Larsen\n",
    "- Alejandro Servin\n",
    "- Lily Steiner\n",
    "- Mayra Trejo\n",
    "- Lucy Lennemann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='research_question'></a>\n",
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How has the sentiment of the language surrounding Deafness used by popular online news sources (ABC, New York Times, USA Today, The Guardian, Associated Press) changed since the 80s?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages, some will be used during analysis\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import unicodedata\n",
    "import nltk\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datasets\n",
    "# Import ABC Dataset\n",
    "with open('dataset/abc_data.json') as abc_ds:\n",
    "    abc_data=json.load(abc_ds)\n",
    "    \n",
    "# Import Alternative Press Dataset\n",
    "with open('dataset/ap_data.json') as ap_ds:\n",
    "    ap_data=json.load(ap_ds)\n",
    "\n",
    "# Import The Guardian Dataset\n",
    "with open('dataset/guard_data.json') as guard_ds:\n",
    "    guard_data=json.load(guard_ds)\n",
    "    \n",
    "# Import New York Times Dataset\n",
    "with open('dataset/nyt_data.json') as nyt_ds:\n",
    "    nyt_data=json.load(nyt_ds)\n",
    "\n",
    "# Import USA Today Dataset\n",
    "with open('dataset/usa_data.json') as usa_ds:\n",
    "    usa_data=json.load(usa_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datasets to dataforms\n",
    "abc_df = pd.read_json('dataset/abc_data.json')\n",
    "ap_df = pd.read_json('dataset/ap_data.json') \n",
    "guard_df = pd.read_json('dataset/guard_data.json')\n",
    "nyt_df = pd.read_json('dataset/nyt_data.json') \n",
    "usa_df = pd.read_json('dataset/usa_data.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set row and column display\n",
    "pd.options.display.max_rows=6\n",
    "pd.options.display.max_columns=5\n",
    "\n",
    "#Used to look for text errors reverted for cleaning\n",
    "#pd.options.display.max_colwidth=None \n",
    "\n",
    "pd.options.display.max_colwidth=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Space for textblob coode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe your data cleaning steps here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We are reordering the columns of all the dataframes so that they match.\n",
    "2. We are converting the date strings into pd.datetime format\n",
    "3. We are removing all articles before 1980-01-01\n",
    "4. We are removing unicode artifacts from the text using unicodedata.normalize\n",
    "5. We are removing any extraneous articles\n",
    "6. We are removing any extraneous pieces of article text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize dataframe\n",
    "abc_df                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize columns\n",
    "abc_df = abc_df[['headline','date','source','url','text']]\n",
    "\n",
    "# Convert 'date' to datetime format and only visualize date\n",
    "abc_df['date'] = pd.to_datetime(abc_df['date'], errors='coerce')\n",
    "\n",
    "# Remove articles before 1980-01-01\n",
    "abc_df = abc_df[~(abc_df['date']<='1980-01-01')]\n",
    "\n",
    "# Drop 'source' column for easier visualization\n",
    "abc_df.drop(columns=['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for null values\n",
    "abc_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb for unique values in the 'headline' column\n",
    "abc_df['headline'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb text for unique values in the 'text' column\n",
    "abc_df['text'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean text\n",
    "abc_df['text'] = abc_df['text'].apply(lambda t: unicodedata.normalize('NFKD', t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Press Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize dataframe\n",
    "ap_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize columns\n",
    "ap_df = ap_df[['headline','date','source','url','text']]\n",
    "\n",
    "# Convert 'date' to datetime format and only visualize date\n",
    "ap_df['date'] = pd.to_datetime(ap_df['date'])\n",
    "\n",
    "#Remove articles before 1980-01-01\n",
    "ap_df = ap_df[~(ap_df['date']<='1980-01-01')]\n",
    "\n",
    "# Drop 'source' column for easier visualization\n",
    "ap_df.drop(columns=['source'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for null values\n",
    "ap_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb for unique values in the 'headline' column\n",
    "ap_df['headline'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb for unique values in the 'text' column\n",
    "ap_df['text'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove articles that report sports scores\n",
    "ap_df = ap_df[ap_df['headline'].str.contains(\"Monday's Scores|Tuesday's Scores|Wednesday's Scores|Thursday's Scores|Friday's Scores|Saturday's Scores|Sunday's Scores\")==False]\n",
    "\n",
    "#Clean text\n",
    "ap_df['text'] = ap_df['text'].apply(lambda t: unicodedata.normalize('NFKD', t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Guardian Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize dataframe\n",
    "guard_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize columns\n",
    "guard_df = guard_df[['headline','date','source','url','text']]\n",
    "\n",
    "# Convert 'date' to datetime format and only visualize date\n",
    "guard_df['date'] = pd.to_datetime(guard_df['date'])\n",
    "\n",
    "#Remove articles before 1980-01-01\n",
    "guard_df = guard_df[~(guard_df['date']<='1980-01-01')]\n",
    "\n",
    "# Drop 'source' column for easier visualization\n",
    "guard_df.drop(columns=['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for null values\n",
    "guard_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb for unique values in the 'headline' column\n",
    "guard_df['headline'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb for unique values in the 'text' column\n",
    "guard_df['text'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean text\n",
    "guard_df['text'] = guard_df['text'].apply(lambda t: unicodedata.normalize('NFKD', t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New York Times Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize dataframe\n",
    "nyt_df                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize columns\n",
    "nyt_df = nyt_df[['headline','date','source','url','text']]\n",
    "\n",
    "# Convert 'date' to datetime format and only visualize date\n",
    "nyt_df['date'] = pd.to_datetime(nyt_df['date'])\n",
    "\n",
    "#Remove articles before 1980-01-01\n",
    "nyt_df = nyt_df[~(nyt_df['date']<='1980-01-01')]\n",
    "\n",
    "# Drop 'source' column for easier visualization\n",
    "nyt_df.drop(columns=['source'])\n",
    "\n",
    "# Visualize 'text' to search for errors\n",
    "#nyt_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look for null values\n",
    "nyt_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb for unique values in the 'headline' column\n",
    "nyt_df['headline'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb for unique values in teh 'text' column\n",
    "nyt_df['text'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USA Today Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe using dataset\n",
    "\n",
    "#visualize dataframe\n",
    "usa_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize columns\n",
    "usa_df = usa_df[['headline','date','source','url','text']]\n",
    "\n",
    "# Convert 'date' to datetime format and only visualize date\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "usa_df['date'] = usa_df['date'].str.extract(r'Published:? (.*?)(?:Updated:?.*)?$')\n",
    "usa_df['date'] = usa_df['date'].str.replace('ET', '')\n",
    "usa_df['date'] = pd.to_datetime(usa_df['date'])\n",
    "\n",
    "pd.options.mode.chained_assignment = 'warn'\n",
    "\n",
    "# Remove articles before 1980-01-01\n",
    "usa_df = usa_df[~(usa_df['date']<='1980-01-01')]\n",
    "\n",
    "# Drop 'source' column for easier visualization\n",
    "usa_df.drop(columns=['source'])\n",
    "\n",
    "#Find data types\n",
    "usa_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for null values\n",
    "print(usa_df.isnull().sum())\n",
    "usa_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb for unique values in the 'headline' column\n",
    "usa_df['headline'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comb for unique values in the 'text' column\n",
    "usa_df['text'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Text\n",
    "usa_df['text'] = usa_df['text'].apply(lambda t: unicodedata.normalize('NFKD', t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes for function iteration\n",
    "df_list = [abc_df, ap_df, guard_df, nyt_df, usa_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat(df_list)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis & Results (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carry out EDA on your dataset(s); Describe in this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next section, we are creating new dataframes which will have the analysis results in addition to defining a few helper functions for our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_sent = abc_df\n",
    "ap_sent = ap_df\n",
    "guard_sent = guard_df\n",
    "nyt_sent = nyt_df\n",
    "usa_sent = usa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find sentiment for a given piece of text\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity, subjectivity = blob.sentiment\n",
    "    return polarity, subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleans text and returns textblob object for keyword analysis \n",
    "def cleaned_blob(text):    \n",
    "    #removes all quotations, periods, commas, and hyphens\n",
    "    text = text.replace('‘', '')\n",
    "    text = text.replace('’', '')\n",
    "    text = text.replace('“', '')\n",
    "    text = text.replace('”', '')\n",
    "    text = text.replace('.', ' ')\n",
    "    text = text.replace(',', ' ')\n",
    "    text = text.replace('–', ' ')   \n",
    "    text = text.replace('-', ' ')\n",
    "    #removes stopwords \n",
    "    words_list = (x for x in TextBlob(text).words if x not in stopwords.words('English'))\n",
    "    #removes numbers, not relevant for keyword analysis\n",
    "    words_list = (x for x in words_list if x.isalpha())\n",
    "    #lemmatizes\n",
    "    words_list = (Word(word).lemmatize() for word in words_list)\n",
    "    # joins all words into one string\n",
    "    cleaned = ' '.join(words_list)\n",
    "    b = TextBlob(cleaned) \n",
    "    #remove leading/trailing whitespace and makes all lowercase\n",
    "    b = b.strip()\n",
    "    b = b.lower()\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply this function to each news dataframe and add two columns with the objectivity score and subjectivity score (both ranging from -1 to 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data_sent = [abc_sent, ap_sent, guard_sent, nyt_sent, usa_sent]\n",
    "for df in news_data_sent:\n",
    "    df[['polarity', 'subjectivity']]=df.apply(lambda x: get_sentiment(x['text']),axis=1,\n",
    "                             result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test to see if properly configured\n",
    "abc_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
